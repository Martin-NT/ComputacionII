# COMPUTACI√ìN II

## TP2 - Sistema de Scraping y An√°lisis Web Distribuido

**Fecha de entrega: 14/11/2025**

### Informaci√≥n   
- **Nombre:** Martin Navarro
- **Legajo:** 62181
- **Correo:** mt.navarro@alumno.um.edu.ar
- **Materia**: Computaci√≥n II

---

## üìÅ Estructura del Proyecto

| Archivo | Descripci√≥n |
|--------|-------------|
| `server_scraping.py` | Servidor A: Servidor HTTP (asyncio) que recibe peticiones del cliente. |
| `server_processing.py` | Servidor B: Servidor TCP (multiprocessing) que realiza el trabajo pesado.. |
| `common/protocol.py` | Protocolo de sockets para client.py. |
| `common/serialization.py` | Funciones auxiliares para serializaci√≥n JSON. |
| `scraper/async_http.py` | Cliente HTTP as√≠ncrono (aiohttp) para descargar HTML. |
| `scraper/html_parser.py` | Extrae t√≠tulo, links, headers e im√°genes del HTML. |
| `scraper/metadata_extractor.py` | Extrae las meta tags (description, keywords, etc.). |
| `processor/screenshot.py` | Genera el screenshot de la p√°gina web (Playwright) |
| `processor/performance.py` | Calcula el rendimiento de la p√°gina (requests). |
| `processor/image_processor.py` | Descarga im√°genes y genera thumbnails (Pillow). |
| `client.py` | Cliente de prueba simple para conectar directo al Servidor B (Socket). |
| `client2.py` | Cliente de prueba completo para conectar al Servidor A (HTTP). |
| `tests/test_processor.py` | Test de integraci√≥n para el Servidor B. |
| `tests/test_scraper.py` | Test de integraci√≥n para el Servidor A. |
| `requirements.txt` | Lista de dependencias de Python necesarias (ej: aiohttp, playwright). |
| `.gitignore` | Archivos y carpetas excluidos del repositorio (ej: `env/`, `__pycache__/`). |
| `README.md` | Este archivo con toda la informaci√≥n del proyecto. |

## üì¶ Requisitos

- Python 3.8 o superior
- (Recomendado) Uso de entorno virtual (venv)
- Instalar dependencias definidas en requirements.txt (Paso 5 de Instalaci√≥n)

## 1. Instalaci√≥n

1. Clonar el repositorio
```bash
git clone git@github.com:Martin-NT/ComputacionII.git
```

2. Ubicarse en el proyecto
```bash
cd ComputacionII/TP2
```

3. Crear entorno virtual
```bash
python3 -m venv env
```

4. Activar entorno virtual
```bash
source env/bin/activate
```

5. Instalar dependencias
```bash
pip install -r requirements.txt
```
6. Instalar Navegadores (Playwright)
```bash
playwright install chromium
```

## 2. Ejecucion del Sistema
Para que el sistema funcione, debes tener **ambos servidores corriendo** al mismo tiempo en dos terminales separadas. Aseg√∫rate de tener el entorno virtual (`venv`) activado en ambas.

### Terminal 1: Iniciar Servidor B
```bash
python3 server_processing.py -i 127.0.0.1 -p 9090 -n 2
```
- **Nota:** Si omites el `-n 2`, el servidor usar√° por defecto todos los n√∫cleos de tu CPU (`mp.cpu_count()`), no del navegador.
- **Salida esperada:** `[B] listening on 127.0.0.1:9090 (pool=2)`

### üíª Terminal 2: Iniciar Servidor A (Scraping)

Este es el servidor principal que atiende a los clientes. Escucha en el puerto `8000` y se conecta al Servidor B en el puerto `9090`.
```bash
python3 server_scraping.py -i 0.0.0.0 -p 8000 --proc-ip 127.0.0.1 --proc-port 9090
```
- **Salida esperada:** `======== Running on http://0.0.0.0:8000 ========`

## 3. Comandos de Prueba

Con los dos servidores corriendo, abre una **tercera terminal** (con el entorno activado) para ejecutar las pruebas.

### Prueba A: Cliente Python Completo (Recomendado)

El script `client2.py` (el cliente HTTP mejorado) prueba toda la arquitectura:
1.  Se conecta al **Servidor A** (puerto 8000).
2.  Imprime el JSON completo que recibe de `https://example.com`.
3.  Decodifica y guarda la imagen `screenshot_cliente.png`.

**Comando:**
```bash
python3 client2.py
```

### Prueba B: Probar S√ìLO el Servidor B

El script `client.py` original prueba la conexi√≥n directa por socket al **Servidor B** (puerto 9090).

**Comando:**
```bash
python3 client.py
```
- **Salida Esperada:** `ok ['screenshot', 'performance', 'thumbnails']`

### Prueba C: Probar con `curl` 

#### Para ver JSON de otra URL
Si quieres probar r√°pidamente con **otra URL** y solo ver el JSON en la terminal.

```bash
curl -s "http://127.0.0.1:8000/scrape?url=https://www.google.com" | python3 -m json.tool
```

#### Para guardar imagen de otra URL
Si quieres probar con **otra URL** y guardar su screenshot directamente.

```bash
curl -s "http://127.0.0.1:8000/scrape?url=https://www.google.com" | python3 -c "import sys, json, base64; data=json.load(sys.stdin); img_data=base64.b64decode(data['processing_data']['screenshot']); sys.stdout.buffer.write(img_data)" > google_screenshot.png 
```

## 4. Correr los Tests
Para que el sistema funcione, debes tener **ambos servidores corriendo** al mismo tiempo en dos terminales separadas. Aseg√∫rate de tener el entorno virtual (`venv`) activado en ambas.

# Prueba del Servidor B
- **Requiere:** Que el **Servidor B** (`server_processing.py`) est√© corriendo en la Terminal 1 y el entorno virtual activado en la nueva terminal.
- **Comando:**
    ```bash
    python3 tests/test_processor.py
    ```
- **Salida esperada:** `‚úÖ PRUEBA SERVIDOR B: EXITOSA.`

# Prueba del Servidor A (y la integraci√≥n con B)
- **Requiere:** Que **AMBOS servidores** (A y B) est√©n corriendo en sus terminales y el entorno virtual activado en la nueva terminal.
- **Comando:**
    ```bash
    python3 tests/test_scraper.py
    ```
- **Salida esperada:** `‚úÖ PRUEBA SERVIDOR A (INTEGRACI√ìN): EXITOSA.`
---

## **Problema**

Se requiere desarrollar un sistema distribuido de scraping y an√°lisis web utilizando Python. El sistema debe consistir en dos servidores que trabajan de forma coordinada para extraer, analizar y procesar informaci√≥n de sitios web.

---

### **Parte A: Servidor de Extracci√≥n As√≠ncrono**

Implementar un servidor HTTP utilizando **asyncio** que maneje las solicitudes de scraping de forma as√≠ncrona. Este servidor debe:

- Recibir URLs de sitios web a analizar a trav√©s de peticiones HTTP
- Realizar el scraping de la p√°gina web de forma as√≠ncrona sin bloquear el event loop
- Extraer la siguiente informaci√≥n de cada URL:
  - T√≠tulo de la p√°gina
  - Todos los enlaces (links) encontrados
  - Meta tags relevantes (description, keywords, Open Graph tags)
  - Cantidad de im√°genes en la p√°gina
  - Estructura b√°sica (cantidad de headers H1-H6)
- Comunicarse con el servidor de procesamiento (Parte B) para solicitar an√°lisis adicional
- Esperar de forma as√≠ncrona los resultados del servidor de procesamiento
- Devolver al cliente una respuesta JSON consolidada con toda la informaci√≥n extra√≠da y procesada

El servidor debe implementar mecanismos de comunicaci√≥n as√≠ncrona entre tareas para coordinar el scraping y el procesamiento sin bloquear operaciones.

---

### **Parte B: Servidor de Procesamiento con Multiprocessing**

Implementar un servidor utilizando **multiprocessing** y **socketserver** que procese tareas computacionalmente intensivas. Este servidor debe:

- Escuchar conexiones en un puerto diferente al servidor principal
- Recibir solicitudes del Servidor A a trav√©s de sockets
- Ejecutar las siguientes operaciones en procesos separados:
  - **Captura de screenshot**: Generar una imagen (PNG) de c√≥mo se ve la p√°gina web renderizada
  - **An√°lisis de rendimiento**: Calcular el tiempo de carga, tama√±o total de recursos, cantidad de requests necesarios
  - **An√°lisis de im√°genes**: Descargar las im√°genes principales de la p√°gina y generar thumbnails optimizados
- Manejar m√∫ltiples solicitudes concurrentemente utilizando un pool de procesos
- Devolver los resultados al Servidor A a trav√©s del socket

La comunicaci√≥n entre ambos servidores debe realizarse mediante sockets y utilizar serializaci√≥n apropiada (JSON, pickle, o protocol buffers).

---

### **Parte C: Transparencia para el Cliente**

El cliente debe interactuar √∫nicamente con el Servidor A (asyncio). Todas las operaciones de procesamiento realizadas por el Servidor B deben ser completamente transparentes para el cliente. 

El servidor A debe:
- Recibir la petici√≥n del cliente
- Coordinar autom√°ticamente con el Servidor B cuando sea necesario
- Consolidar todos los resultados
- Devolver una respuesta √∫nica al cliente

Desde la perspectiva del cliente, todo el procesamiento debe parecer que ocurre en un solo servidor.

---

## **Requerimientos T√©cnicos**

### **Funcionalidad M√≠nima**

- La aplicaci√≥n debe contener como m√≠nimo **4 funciones principales**:
  1. Scraping de contenido HTML
  2. Extracci√≥n de metadatos
  3. Generaci√≥n de screenshot
  4. An√°lisis de rendimiento

### **Networking**

- El Servidor A debe soportar conexiones **IPv4 e IPv6** indistintamente
- Implementar manejo de errores de red (timeouts, conexiones rechazadas, etc.)
- El Servidor B debe escuchar en un puerto diferente y manejar protocolo de comunicaci√≥n binario eficiente

### **Concurrencia y Paralelismo**

- Uso obligatorio de **asyncio** para el Servidor A:
  - Manejo as√≠ncrono de m√∫ltiples clientes
  - Requests HTTP as√≠ncronos (usar `aiohttp`)
  - Comunicaci√≥n as√≠ncrona entre componentes
  
- Uso obligatorio de **multiprocessing** para el Servidor B:
  - Pool de workers para procesamiento paralelo
  - Manejo de tareas CPU-bound en procesos separados
  - Sincronizaci√≥n apropiada entre procesos

### **Interfaz de L√≠nea de Comandos**

Implementar parsing de argumentos con **argparse** (o **getopt**):

```bash
# Servidor Principal (Parte A)
$ ./server_scraping.py -h
usage: server_scraping.py [-h] -i IP -p PORT [-w WORKERS]

Servidor de Scraping Web As√≠ncrono

Opciones:
  -h, --help            Muestra este mensaje de ayuda
  -i IP, --ip IP        Direcci√≥n de escucha (soporta IPv4/IPv6)
  -p PORT, --port PORT  Puerto de escucha
  -w WORKERS, --workers WORKERS
                        N√∫mero de workers (default: 4)

# Servidor de Procesamiento (Parte B)
$ ./server_processing.py -h
usage: server_processing.py [-h] -i IP -p PORT [-n PROCESSES]

Servidor de Procesamiento Distribuido

Opciones:
  -h, --help            Muestra este mensaje de ayuda
  -i IP, --ip IP        Direcci√≥n de escucha
  -p PORT, --port PORT  Puerto de escucha
  -n PROCESSES, --processes PROCESSES
                        N√∫mero de procesos en el pool (default: CPU count)
```

### **Manejo de Errores**

Implementar manejo robusto de errores:
- URLs inv√°lidas o inaccesibles
- Timeouts en scraping (m√°ximo 30 segundos por p√°gina)
- Errores de comunicaci√≥n entre servidores
- Recursos no disponibles (im√°genes, CSS, etc.)
- L√≠mites de memoria para p√°ginas muy grandes

### **Formato de Respuesta**

El servidor debe devolver un JSON con la siguiente estructura:

```json
{
  "url": "https://ejemplo.com",
  "timestamp": "2024-11-10T15:30:00Z",
  "scraping_data": {
    "title": "T√≠tulo de la p√°gina",
    "links": ["url1", "url2", "..."],
    "meta_tags": {
      "description": "...",
      "keywords": "...",
      "og:title": "..."
    },
    "structure": {
      "h1": 2,
      "h2": 5,
      "h3": 10
    },
    "images_count": 15
  },
  "processing_data": {
    "screenshot": "base64_encoded_image",
    "performance": {
      "load_time_ms": 1250,
      "total_size_kb": 2048,
      "num_requests": 45
    },
    "thumbnails": ["base64_thumb1", "base64_thumb2"]
  },
  "status": "success"
}
```

---

## **Bonus Track** (Puntos Extra)

### **Opci√≥n 1: Sistema de Cola con IDs de Tarea**

Implementar un sistema as√≠ncrono de cola de trabajos:

- El servidor A devuelve inmediatamente un **task_id** al cliente
- El cliente puede consultar el estado de la tarea con ese ID:
  ```
  GET /status/{task_id}
  ```
- Estados posibles: `pending`, `scraping`, `processing`, `completed`, `failed`
- Cuando est√© completa, el cliente puede descargar los resultados:
  ```
  GET /result/{task_id}
  ```

### **Opci√≥n 2: Rate Limiting y Cach√©**

- Implementar rate limiting por dominio (m√°ximo N requests por minuto al mismo dominio)
- Sistema de cach√©: si una URL ya fue scrapeada recientemente (< 1 hora), devolver resultado cacheado
- Usar Redis o un diccionario compartido con TTL

### **Opci√≥n 3: An√°lisis Avanzado**

Agregar an√°lisis adicional en el Servidor B:
- Detecci√≥n de tecnolog√≠as usadas (frameworks JS, CMS, etc.)
- An√°lisis de SEO (score de optimizaci√≥n)
- Extracci√≥n de esquemas estructurados (JSON-LD, Schema.org)
- An√°lisis de accesibilidad (contraste de colores, alt tags)

---

## **Objetivos de Aprendizaje**

Al completar este trabajo pr√°ctico, el estudiante ser√° capaz de:

1. **Programaci√≥n As√≠ncrona**:
   - Dise√±ar y implementar servidores con asyncio
   - Manejar I/O no bloqueante eficientemente
   - Coordinar m√∫ltiples tareas as√≠ncronas concurrentes

2. **Programaci√≥n Paralela**:
   - Utilizar multiprocessing para tareas CPU-bound
   - Dise√±ar arquitecturas con pools de procesos
   - Implementar comunicaci√≥n inter-proceso (IPC)

3. **Networking**:
   - Implementar protocolos de comunicaci√≥n cliente-servidor
   - Manejar sockets TCP para comunicaci√≥n entre servicios
   - Soportar IPv4 e IPv6

4. **Web Scraping**:
   - Extraer informaci√≥n de p√°ginas web usando BeautifulSoup/lxml
   - Manejar diferentes estructuras HTML
   - Procesar y normalizar datos extra√≠dos

5. **Arquitectura de Sistemas Distribuidos**:
   - Dise√±ar sistemas con m√∫ltiples componentes independientes
   - Implementar transparencia de distribuci√≥n
   - Manejar fallos y errores en sistemas distribuidos

---

## **Dependencias Requeridas**

```bash
pip install aiohttp beautifulsoup4 lxml Pillow selenium aiofiles
```

Opcional para screenshots:
```bash
# ChromeDriver o GeckoDriver para Selenium
# O usar playwright: pip install playwright && playwright install
```

---

## **Estructura de Proyecto Sugerida**

```
TP2/
‚îú‚îÄ‚îÄ server_scraping.py          # Servidor asyncio (Parte A)
‚îú‚îÄ‚îÄ server_processing.py        # Servidor multiprocessing (Parte B)
‚îú‚îÄ‚îÄ client.py                   # Cliente de prueba
‚îú‚îÄ‚îÄ scraper/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ html_parser.py          # Funciones de parsing HTML
‚îÇ   ‚îú‚îÄ‚îÄ metadata_extractor.py  # Extracci√≥n de metadatos
‚îÇ   ‚îî‚îÄ‚îÄ async_http.py           # Cliente HTTP as√≠ncrono
‚îú‚îÄ‚îÄ processor/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ screenshot.py           # Generaci√≥n de screenshots
‚îÇ   ‚îú‚îÄ‚îÄ performance.py          # An√°lisis de rendimiento
‚îÇ   ‚îî‚îÄ‚îÄ image_processor.py      # Procesamiento de im√°genes
‚îú‚îÄ‚îÄ common/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ protocol.py             # Protocolo de comunicaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ serialization.py        # Serializaci√≥n de datos
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_scraper.py
‚îÇ   ‚îî‚îÄ‚îÄ test_processor.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## **Criterios de Evaluaci√≥n**

| Criterio | Puntos | Descripci√≥n |
|----------|--------|-------------|
| Funcionalidad completa | 30 | Todas las partes A, B y C funcionan correctamente |
| Uso correcto de asyncio | 20 | Servidor as√≠ncrono eficiente, no hay bloqueos del event loop |
| Uso correcto de multiprocessing | 20 | Pool de procesos funcionando, IPC correcta |
| Manejo de errores | 10 | Errores manejados apropiadamente en todos los casos |
| Calidad de c√≥digo | 10 | C√≥digo limpio, modular, bien documentado |
| Interfaz CLI | 5 | Argumentos parseados correctamente, ayuda clara |
| Bonus Track | +15 | Implementaci√≥n de features opcionales |

**Total**: 95 puntos (+ 15 bonus)

---

## **Referencias**

- **AsyncIO Documentation**: [https://docs.python.org/3/library/asyncio.html](https://docs.python.org/3/library/asyncio.html)
- **aiohttp Documentation**: [https://docs.aiohttp.org/](https://docs.aiohttp.org/)
- **Multiprocessing Documentation**: [https://docs.python.org/3/library/multiprocessing.html](https://docs.python.org/3/library/multiprocessing.html)
- **SocketServer Documentation**: [https://docs.python.org/3/library/socketserver.html](https://docs.python.org/3/library/socketserver.html)
- **BeautifulSoup Documentation**: [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- **Selenium Documentation**: [https://selenium-python.readthedocs.io/](https://selenium-python.readthedocs.io/)

---

## **Instrucciones de Entrega**

1. **Repositorio**: Subir el proyecto al repositorio personal de GitHub/GitLab
2. **Carpeta**: Debe estar en una carpeta llamada exactamente `TP2`
3. **README**: Incluir instrucciones de instalaci√≥n y ejecuci√≥n
4. **Fecha l√≠mite**: 14/11/2025 - 23:59 hs

---

## **Consejos y Recomendaciones**

### **Para el Servidor Asyncio**

- Usa `aiohttp.ClientSession` con timeout configurado
- No uses `requests` (es bloqueante), siempre `aiohttp`
- Implementa un l√≠mite de conexiones concurrentes por dominio
- Maneja excepciones espec√≠ficas de asyncio (`asyncio.TimeoutError`, etc.)

### **Para el Servidor Multiprocessing**

- Usa `ProcessPoolExecutor` o `Pool` de multiprocessing
- Ten cuidado con objetos no serializables (no se pueden pasar entre procesos)
- Usa `Queue` o `Pipe` para comunicaci√≥n inter-proceso si es necesario
- Cierra el pool apropiadamente con context managers

### **Para Screenshots**

- Considera usar modo headless para eficiencia
- Configura timeouts para p√°ginas que tardan mucho
- Maneja p√°ginas que requieren JavaScript vs HTML est√°tico

### **Para la Comunicaci√≥n entre Servidores**

- Define un protocolo claro (ej: longitud del mensaje + mensaje)
- Serializa con `json` o `pickle` seg√∫n necesidad
- Implementa reintentos en caso de fallo de comunicaci√≥n
- Considera usar `asyncio.open_connection` para sockets as√≠ncronos

### **Testing**

```python
# Ejemplo de test
import asyncio
import aiohttp

async def test_scraper():
    async with aiohttp.ClientSession() as session:
        async with session.get('http://localhost:8000/scrape?url=https://example.com') as resp:
            data = await resp.json()
            assert 'scraping_data' in data
            assert data['status'] == 'success'

asyncio.run(test_scraper())
```

---

¬°√âxito con el trabajo pr√°ctico! Recuerda comenzar temprano y testear frecuentemente cada componente de forma independiente antes de integrar todo el sistema.

---